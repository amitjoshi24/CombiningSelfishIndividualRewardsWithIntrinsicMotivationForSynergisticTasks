{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFo8Ef-LEThV"
      },
      "outputs": [],
      "source": [
        "!sudo apt install curl git libgl1-mesa-dev libgl1-mesa-glx libglew-dev \\\n",
        "        libosmesa6-dev software-properties-common net-tools unzip vim \\\n",
        "        virtualenv wget xpra xserver-xorg-dev libglfw3-dev patchelf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQOgBKhqFWMa"
      },
      "outputs": [],
      "source": [
        "!pip install robosuite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZNbG3BWSliM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import robosuite as suite\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as D\n",
        "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z0yREBZsAlD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import os.path as osp\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "import tempfile\n",
        "from collections import defaultdict\n",
        "from contextlib import contextmanager\n",
        "\n",
        "DEBUG = 10\n",
        "INFO = 20\n",
        "WARN = 30\n",
        "ERROR = 40\n",
        "\n",
        "DISABLED = 50\n",
        "\n",
        "class KVWriter(object):\n",
        "    def writekvs(self, kvs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class SeqWriter(object):\n",
        "    def writeseq(self, seq):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class HumanOutputFormat(KVWriter, SeqWriter):\n",
        "    def __init__(self, filename_or_file):\n",
        "        if isinstance(filename_or_file, str):\n",
        "            self.file = open(filename_or_file, 'wt')\n",
        "            self.own_file = True\n",
        "        else:\n",
        "            assert hasattr(filename_or_file, 'read'), 'expected file or str, got %s'%filename_or_file\n",
        "            self.file = filename_or_file\n",
        "            self.own_file = False\n",
        "\n",
        "    def writekvs(self, kvs):\n",
        "        # Create strings for printing\n",
        "        key2str = {}\n",
        "        for (key, val) in sorted(kvs.items()):\n",
        "            if hasattr(val, '__float__'):\n",
        "                valstr = '%-8.3g' % val\n",
        "            else:\n",
        "                valstr = str(val)\n",
        "            key2str[self._truncate(key)] = self._truncate(valstr)\n",
        "\n",
        "        # Find max widths\n",
        "        if len(key2str) == 0:\n",
        "            print('WARNING: tried to write empty key-value dict')\n",
        "            return\n",
        "        else:\n",
        "            keywidth = max(map(len, key2str.keys()))\n",
        "            valwidth = max(map(len, key2str.values()))\n",
        "\n",
        "        # Write out the data\n",
        "        dashes = '-' * (keywidth + valwidth + 7)\n",
        "        lines = [dashes]\n",
        "        for (key, val) in sorted(key2str.items(), key=lambda kv: kv[0].lower()):\n",
        "            lines.append('| %s%s | %s%s |' % (\n",
        "                key,\n",
        "                ' ' * (keywidth - len(key)),\n",
        "                val,\n",
        "                ' ' * (valwidth - len(val)),\n",
        "            ))\n",
        "        lines.append(dashes)\n",
        "        self.file.write('\\n'.join(lines) + '\\n')\n",
        "\n",
        "        # Flush the output to the file\n",
        "        self.file.flush()\n",
        "\n",
        "    def _truncate(self, s):\n",
        "        maxlen = 30\n",
        "        return s[:maxlen-3] + '...' if len(s) > maxlen else s\n",
        "\n",
        "    def writeseq(self, seq):\n",
        "        seq = list(seq)\n",
        "        for (i, elem) in enumerate(seq):\n",
        "            self.file.write(elem)\n",
        "            if i < len(seq) - 1: # add space unless this is the last one\n",
        "                self.file.write(' ')\n",
        "        self.file.write('\\n')\n",
        "        self.file.flush()\n",
        "\n",
        "    def close(self):\n",
        "        if self.own_file:\n",
        "            self.file.close()\n",
        "\n",
        "class JSONOutputFormat(KVWriter):\n",
        "    def __init__(self, filename):\n",
        "        self.file = open(filename, 'wt')\n",
        "\n",
        "    def writekvs(self, kvs):\n",
        "        for k, v in sorted(kvs.items()):\n",
        "            if hasattr(v, 'dtype'):\n",
        "                kvs[k] = float(v)\n",
        "        self.file.write(json.dumps(kvs) + '\\n')\n",
        "        self.file.flush()\n",
        "\n",
        "    def close(self):\n",
        "        self.file.close()\n",
        "\n",
        "class CSVOutputFormat(KVWriter):\n",
        "    def __init__(self, filename):\n",
        "        self.file = open(filename, 'w+t')\n",
        "        self.keys = []\n",
        "        self.sep = ','\n",
        "\n",
        "    def writekvs(self, kvs):\n",
        "        # Add our current row to the history\n",
        "        extra_keys = list(kvs.keys() - self.keys)\n",
        "        extra_keys.sort()\n",
        "        if extra_keys:\n",
        "            self.keys.extend(extra_keys)\n",
        "            self.file.seek(0)\n",
        "            lines = self.file.readlines()\n",
        "            self.file.seek(0)\n",
        "            for (i, k) in enumerate(self.keys):\n",
        "                if i > 0:\n",
        "                    self.file.write(',')\n",
        "                self.file.write(k)\n",
        "            self.file.write('\\n')\n",
        "            for line in lines[1:]:\n",
        "                self.file.write(line[:-1])\n",
        "                self.file.write(self.sep * len(extra_keys))\n",
        "                self.file.write('\\n')\n",
        "        for (i, k) in enumerate(self.keys):\n",
        "            if i > 0:\n",
        "                self.file.write(',')\n",
        "            v = kvs.get(k)\n",
        "            if v is not None:\n",
        "                self.file.write(str(v))\n",
        "        self.file.write('\\n')\n",
        "        self.file.flush()\n",
        "\n",
        "    def close(self):\n",
        "        self.file.close()\n",
        "\n",
        "\n",
        "class TensorBoardOutputFormat(KVWriter):\n",
        "    \"\"\"\n",
        "    Dumps key/value pairs into TensorBoard's numeric format.\n",
        "    \"\"\"\n",
        "    def __init__(self, dir):\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "        self.dir = dir\n",
        "        self.step = 1\n",
        "        prefix = 'events'\n",
        "        path = osp.join(osp.abspath(dir), prefix)\n",
        "        import tensorflow as tf\n",
        "        from tensorflow.python import pywrap_tensorflow\n",
        "        from tensorflow.core.util import event_pb2\n",
        "        from tensorflow.python.util import compat\n",
        "        self.tf = tf\n",
        "        self.event_pb2 = event_pb2\n",
        "        self.pywrap_tensorflow = pywrap_tensorflow\n",
        "        self.writer = pywrap_tensorflow.EventsWriter(compat.as_bytes(path))\n",
        "\n",
        "    def writekvs(self, kvs):\n",
        "        def summary_val(k, v):\n",
        "            kwargs = {'tag': k, 'simple_value': float(v)}\n",
        "            return self.tf.Summary.Value(**kwargs)\n",
        "        summary = self.tf.Summary(value=[summary_val(k, v) for k, v in kvs.items()])\n",
        "        event = self.event_pb2.Event(wall_time=time.time(), summary=summary)\n",
        "        event.step = self.step # is there any reason why you'd want to specify the step?\n",
        "        self.writer.WriteEvent(event)\n",
        "        self.writer.Flush()\n",
        "        self.step += 1\n",
        "\n",
        "    def close(self):\n",
        "        if self.writer:\n",
        "            self.writer.Close()\n",
        "            self.writer = None\n",
        "\n",
        "def make_output_format(format, ev_dir, log_suffix=''):\n",
        "    os.makedirs(ev_dir, exist_ok=True)\n",
        "    if format == 'stdout':\n",
        "        return HumanOutputFormat(sys.stdout)\n",
        "    elif format == 'log':\n",
        "        return HumanOutputFormat(osp.join(ev_dir, 'log%s.txt' % log_suffix))\n",
        "    elif format == 'json':\n",
        "        return JSONOutputFormat(osp.join(ev_dir, 'progress%s.json' % log_suffix))\n",
        "    elif format == 'csv':\n",
        "        return CSVOutputFormat(osp.join(ev_dir, 'progress%s.csv' % log_suffix))\n",
        "    elif format == 'tensorboard':\n",
        "        return TensorBoardOutputFormat(osp.join(ev_dir, 'tb%s' % log_suffix))\n",
        "    else:\n",
        "        raise ValueError('Unknown format specified: %s' % (format,))\n",
        "\n",
        "# ================================================================\n",
        "# API\n",
        "# ================================================================\n",
        "\n",
        "def logkv(key, val):\n",
        "    \"\"\"\n",
        "    Log a value of some diagnostic\n",
        "    Call this once for each diagnostic quantity, each iteration\n",
        "    If called many times, last value will be used.\n",
        "    \"\"\"\n",
        "    get_current().logkv(key, val)\n",
        "\n",
        "def logkv_mean(key, val):\n",
        "    \"\"\"\n",
        "    The same as logkv(), but if called many times, values averaged.\n",
        "    \"\"\"\n",
        "    get_current().logkv_mean(key, val)\n",
        "\n",
        "def logkvs(d):\n",
        "    \"\"\"\n",
        "    Log a dictionary of key-value pairs\n",
        "    \"\"\"\n",
        "    for (k, v) in d.items():\n",
        "        logkv(k, v)\n",
        "\n",
        "def dumpkvs():\n",
        "    \"\"\"\n",
        "    Write all of the diagnostics from the current iteration\n",
        "    \"\"\"\n",
        "    return get_current().dumpkvs()\n",
        "\n",
        "def getkvs():\n",
        "    return get_current().name2val\n",
        "\n",
        "\n",
        "def log(*args, level=INFO):\n",
        "    \"\"\"\n",
        "    Write the sequence of args, with no separators, to the console and output files (if you've configured an output file).\n",
        "    \"\"\"\n",
        "    get_current().log(*args, level=level)\n",
        "\n",
        "def debug(*args):\n",
        "    log(*args, level=DEBUG)\n",
        "\n",
        "def info(*args):\n",
        "    log(*args, level=INFO)\n",
        "\n",
        "def warn(*args):\n",
        "    log(*args, level=WARN)\n",
        "\n",
        "def error(*args):\n",
        "    log(*args, level=ERROR)\n",
        "\n",
        "\n",
        "def set_level(level):\n",
        "    \"\"\"\n",
        "    Set logging threshold on current logger.\n",
        "    \"\"\"\n",
        "    get_current().set_level(level)\n",
        "\n",
        "def set_comm(comm):\n",
        "    get_current().set_comm(comm)\n",
        "\n",
        "def get_dir():\n",
        "    \"\"\"\n",
        "    Get directory that log files are being written to.\n",
        "    will be None if there is no output directory (i.e., if you didn't call start)\n",
        "    \"\"\"\n",
        "    return get_current().get_dir()\n",
        "\n",
        "record_tabular = logkv\n",
        "dump_tabular = dumpkvs\n",
        "\n",
        "@contextmanager\n",
        "def profile_kv(scopename):\n",
        "    logkey = 'wait_' + scopename\n",
        "    tstart = time.time()\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        get_current().name2val[logkey] += time.time() - tstart\n",
        "\n",
        "def profile(n):\n",
        "    \"\"\"\n",
        "    Usage:\n",
        "    @profile(\"my_func\")\n",
        "    def my_func(): code\n",
        "    \"\"\"\n",
        "    def decorator_with_name(func):\n",
        "        def func_wrapper(*args, **kwargs):\n",
        "            with profile_kv(n):\n",
        "                return func(*args, **kwargs)\n",
        "        return func_wrapper\n",
        "    return decorator_with_name\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Backend\n",
        "# ================================================================\n",
        "\n",
        "def get_current():\n",
        "    if Logger.CURRENT is None:\n",
        "        _configure_default_logger()\n",
        "\n",
        "    return Logger.CURRENT\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    DEFAULT = None  # A logger with no output files. (See right below class definition)\n",
        "                    # So that you can still log to the terminal without setting up any output files\n",
        "    CURRENT = None  # Current logger being used by the free functions above\n",
        "\n",
        "    def __init__(self, dir, output_formats, comm=None):\n",
        "        self.name2val = defaultdict(float)  # values this iteration\n",
        "        self.name2cnt = defaultdict(int)\n",
        "        self.level = INFO\n",
        "        self.dir = dir\n",
        "        self.output_formats = output_formats\n",
        "        self.comm = comm\n",
        "\n",
        "    # Logging API, forwarded\n",
        "    # ----------------------------------------\n",
        "    def logkv(self, key, val):\n",
        "        self.name2val[key] = val\n",
        "\n",
        "    def logkv_mean(self, key, val):\n",
        "        oldval, cnt = self.name2val[key], self.name2cnt[key]\n",
        "        self.name2val[key] = oldval*cnt/(cnt+1) + val/(cnt+1)\n",
        "        self.name2cnt[key] = cnt + 1\n",
        "\n",
        "    def dumpkvs(self):\n",
        "        if self.comm is None:\n",
        "            d = self.name2val\n",
        "        else:\n",
        "            from baselines.common import mpi_util\n",
        "            d = mpi_util.mpi_weighted_mean(self.comm,\n",
        "                {name : (val, self.name2cnt.get(name, 1))\n",
        "                    for (name, val) in self.name2val.items()})\n",
        "            if self.comm.rank != 0:\n",
        "                d['dummy'] = 1 # so we don't get a warning about empty dict\n",
        "        out = d.copy() # Return the dict for unit testing purposes\n",
        "        for fmt in self.output_formats:\n",
        "            if isinstance(fmt, KVWriter):\n",
        "                fmt.writekvs(d)\n",
        "        self.name2val.clear()\n",
        "        self.name2cnt.clear()\n",
        "        return out\n",
        "\n",
        "    def log(self, *args, level=INFO):\n",
        "        if self.level <= level:\n",
        "            self._do_log(args)\n",
        "\n",
        "    # Configuration\n",
        "    # ----------------------------------------\n",
        "    def set_level(self, level):\n",
        "        self.level = level\n",
        "\n",
        "    def set_comm(self, comm):\n",
        "        self.comm = comm\n",
        "\n",
        "    def get_dir(self):\n",
        "        return self.dir\n",
        "\n",
        "    def close(self):\n",
        "        for fmt in self.output_formats:\n",
        "            fmt.close()\n",
        "\n",
        "    # Misc\n",
        "    # ----------------------------------------\n",
        "    def _do_log(self, args):\n",
        "        for fmt in self.output_formats:\n",
        "            if isinstance(fmt, SeqWriter):\n",
        "                fmt.writeseq(map(str, args))\n",
        "\n",
        "def get_rank_without_mpi_import():\n",
        "    # check environment variables here instead of importing mpi4py\n",
        "    # to avoid calling MPI_Init() when this module is imported\n",
        "    for varname in ['PMI_RANK', 'OMPI_COMM_WORLD_RANK']:\n",
        "        if varname in os.environ:\n",
        "            return int(os.environ[varname])\n",
        "    return 0\n",
        "\n",
        "\n",
        "def baselines_configure(dir=None, format_strs=None, comm=None, log_suffix=''):\n",
        "    \"\"\"\n",
        "    If comm is provided, average all numerical stats across that comm\n",
        "    \"\"\"\n",
        "    if dir is None:\n",
        "        dir = os.getenv('OPENAI_LOGDIR')\n",
        "    if dir is None:\n",
        "        dir = osp.join(tempfile.gettempdir(),\n",
        "            datetime.datetime.now().strftime(\"openai-%Y-%m-%d-%H-%M-%S-%f\"))\n",
        "    assert isinstance(dir, str)\n",
        "    dir = os.path.expanduser(dir)\n",
        "    os.makedirs(os.path.expanduser(dir), exist_ok=True)\n",
        "\n",
        "    rank = get_rank_without_mpi_import()\n",
        "    if rank > 0:\n",
        "        log_suffix = log_suffix + \"-rank%03i\" % rank\n",
        "\n",
        "    if format_strs is None:\n",
        "        if rank == 0:\n",
        "            format_strs = os.getenv('OPENAI_LOG_FORMAT', 'stdout,log,csv').split(',')\n",
        "        else:\n",
        "            format_strs = os.getenv('OPENAI_LOG_FORMAT_MPI', 'log').split(',')\n",
        "    format_strs = filter(None, format_strs)\n",
        "    output_formats = [make_output_format(f, dir, log_suffix) for f in format_strs]\n",
        "\n",
        "    Logger.CURRENT = Logger(dir=dir, output_formats=output_formats, comm=comm)\n",
        "    if output_formats:\n",
        "        log('Logging to %s'%dir)\n",
        "\n",
        "def _configure_default_logger():\n",
        "    baselines_configure()\n",
        "    Logger.DEFAULT = Logger.CURRENT\n",
        "\n",
        "def reset():\n",
        "    if Logger.CURRENT is not Logger.DEFAULT:\n",
        "        Logger.CURRENT.close()\n",
        "        Logger.CURRENT = Logger.DEFAULT\n",
        "        log('Reset logger')\n",
        "\n",
        "@contextmanager\n",
        "def scoped_configure(dir=None, format_strs=None, comm=None):\n",
        "    prevlogger = Logger.CURRENT\n",
        "    baselines_configure(dir=dir, format_strs=format_strs, comm=comm)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        Logger.CURRENT.close()\n",
        "        Logger.CURRENT = prevlogger\n",
        "\n",
        "# ================================================================\n",
        "\n",
        "def _demo():\n",
        "    info(\"hi\")\n",
        "    debug(\"shouldn't appear\")\n",
        "    set_level(DEBUG)\n",
        "    debug(\"should appear\")\n",
        "    dir = \"/tmp/testlogging\"\n",
        "    if os.path.exists(dir):\n",
        "        shutil.rmtree(dir)\n",
        "    baselines_configure(dir=dir)\n",
        "    logkv(\"a\", 3)\n",
        "    logkv(\"b\", 2.5)\n",
        "    dumpkvs()\n",
        "    logkv(\"b\", -2.5)\n",
        "    logkv(\"a\", 5.5)\n",
        "    dumpkvs()\n",
        "    info(\"^^^ should see a = 5.5\")\n",
        "    logkv_mean(\"b\", -22.5)\n",
        "    logkv_mean(\"b\", -44.4)\n",
        "    logkv(\"a\", 5.5)\n",
        "    dumpkvs()\n",
        "    info(\"^^^ should see b = -33.3\")\n",
        "\n",
        "    logkv(\"b\", -2.5)\n",
        "    dumpkvs()\n",
        "\n",
        "    logkv(\"a\", \"longasslongasslongasslongasslongasslongassvalue\")\n",
        "    dumpkvs()\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Readers\n",
        "# ================================================================\n",
        "\n",
        "def read_json(fname):\n",
        "    import pandas\n",
        "    ds = []\n",
        "    with open(fname, 'rt') as fh:\n",
        "        for line in fh:\n",
        "            ds.append(json.loads(line))\n",
        "    return pandas.DataFrame(ds)\n",
        "\n",
        "def read_csv(fname):\n",
        "    import pandas\n",
        "    return pandas.read_csv(fname, index_col=None, comment='#')\n",
        "\n",
        "def read_tb(path):\n",
        "    \"\"\"\n",
        "    path : a tensorboard file OR a directory, where we will find all TB files\n",
        "           of the form events.*\n",
        "    \"\"\"\n",
        "    import pandas\n",
        "    import numpy as np\n",
        "    from glob import glob\n",
        "    import tensorflow as tf\n",
        "    if osp.isdir(path):\n",
        "        fnames = glob(osp.join(path, \"events.*\"))\n",
        "    elif osp.basename(path).startswith(\"events.\"):\n",
        "        fnames = [path]\n",
        "    else:\n",
        "        raise NotImplementedError(\"Expected tensorboard file or directory containing them. Got %s\"%path)\n",
        "    tag2pairs = defaultdict(list)\n",
        "    maxstep = 0\n",
        "    for fname in fnames:\n",
        "        for summary in tf.train.summary_iterator(fname):\n",
        "            if summary.step > 0:\n",
        "                for v in summary.summary.value:\n",
        "                    pair = (summary.step, v.simple_value)\n",
        "                    tag2pairs[v.tag].append(pair)\n",
        "                maxstep = max(summary.step, maxstep)\n",
        "    data = np.empty((maxstep, len(tag2pairs)))\n",
        "    data[:] = np.nan\n",
        "    tags = sorted(tag2pairs.keys())\n",
        "    for (colidx,tag) in enumerate(tags):\n",
        "        pairs = tag2pairs[tag]\n",
        "        for (step, value) in pairs:\n",
        "            data[step-1, colidx] = value\n",
        "    return pandas.DataFrame(data, columns=tags)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    _demo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iKQpB8Juikd"
      },
      "outputs": [],
      "source": [
        "def append_human_init(self, filename_or_file):\n",
        "    if isinstance(filename_or_file, str):\n",
        "        self.file = open(filename_or_file, 'at')\n",
        "        self.own_file = True\n",
        "    else:\n",
        "        assert hasattr(filename_or_file, 'read'), 'expected file or str, got %s'%filename_or_file\n",
        "        self.file = filename_or_file\n",
        "        self.own_file = False\n",
        "\n",
        "def append_json_init(self, filename):\n",
        "    self.file = open(filename, 'at')\n",
        "\n",
        "def append_csv_init(self, filename):\n",
        "    self.file = open(filename, 'a+t')\n",
        "    self.keys = []\n",
        "    self.sep = ','\n",
        "\n",
        "HumanOutputFormat.__init__ = append_human_init\n",
        "JSONOutputFormat.__init__ = append_json_init\n",
        "CSVOutputFormat.__init__ = append_csv_init\n",
        "\n",
        "# create global tensorboardX summary writer.\n",
        "WRITER = None\n",
        "def configure(log_dir, format_strs=None, tbX=False, **kwargs):\n",
        "    global WRITER\n",
        "    if tbX:\n",
        "        tb_dir = os.path.join(log_dir, 'tensorboard')\n",
        "        WRITER = SummaryWriter(tb_dir, **kwargs)\n",
        "    else:\n",
        "        WRITER = None\n",
        "    baselines_configure(log_dir, format_strs)\n",
        "\n",
        "def get_summary_writer():\n",
        "    return WRITER\n",
        "\n",
        "def add_scalar(tag, scalar_value, global_step=None, walltime=None):\n",
        "    assert WRITER is not None, \"call configure to initialize SummaryWriter\"\n",
        "    WRITER.add_scalar(tag, scalar_value, global_step, walltime)\n",
        "    # change interface so both add_scalar and add_scalars adds to the scalar dict.\n",
        "    #WRITER._SummaryWriter__append_to_scalar_dict(tag, scalar_value, global_step, walltime)\n",
        "\n",
        "\"\"\"\n",
        "def add_scalars(main_tag, tag_scalar_dict, global_step=None, walltime=None):\n",
        "    assert WRITER is not None, \"call configure to initialize SummaryWriter\"\n",
        "    WRITER.add_scalars(main_tag, tag_scalar_dict, global_step, walltime)\n",
        "\"\"\"\n",
        "\n",
        "def add_histogram(tag, values, global_step=None, bins='tensorflow'):#, walltime=None, max_bins=None):\n",
        "    assert WRITER is not None, \"call configure to initialize SummaryWriter\"\n",
        "    WRITER.add_histogram(tag, values, global_step, bins)#, walltime, max_bins)\n",
        "\n",
        "def add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW'):\n",
        "    assert WRITER is not None, \"call configure to initialize SummaryWriter\"\n",
        "    WRITER.add_image(tag, img_tensor, global_step, walltime, dataformats)\n",
        "\n",
        "def add_images(tag, img_tensor, global_step=None, walltime=None, dataformats='NCHW'):\n",
        "    assert WRITER is not None, \"call configure to initialize SummaryWriter\"\n",
        "    WRITER.add_images(tag, img_tensor, global_step, walltime, dataformats)\n",
        "\n",
        "def add_image_with_boxes(tag, img_tensor, box_tensor, global_step=None,\n",
        "                             walltime=None, dataformats='CHW', **kwargs):\n",
        "    assert WRITER is not None, \"call configure to initialize SummaryWriter\"\n",
        "    WRITER.add_image_with_boxes(tag, img_tensor, box_tensor, global_step,\n",
        "                                 walltime, dataformats, **kwargs)\n",
        "\n",
        "def add_figure(tag, figure, global_step=None, close=True, walltime=None):\n",
        "    assert WRITER is not None, \"call configure to initialize SummaryWriter\"\n",
        "    WRITER.add_figure(tag, figure, global_step, close, walltime)\n",
        "\n",
        "def add_video(tag, vid_tensor, global_step=None, fps=4, walltime=None):\n",
        "    assert WRITER is not None, \"call configure to initialize SummaryWriter\"\n",
        "    WRITER.add_video(tag, vid_tensor, global_step, fps, walltime)\n",
        "\n",
        "def add_audio(tag, snd_tensor, global_step=None, sample_rate=44100, walltime=None):\n",
        "    assert WRITER is not None, \"call configure to initialize SummaryWriter\"\n",
        "    WRITER.add_audio(tag, snd_tensor, global_step, sample_rate, walltime)\n",
        "\n",
        "def add_text(tag, text_string, global_step=None, walltime=None):\n",
        "    assert WRITER is not None, \"call configure to initialize SummaryWriter\"\n",
        "    WRITER.add_text(tag, text_string, global_step, walltime)\n",
        "\n",
        "def add_graph(model, input_to_model=None, verbose=False, **kwargs):\n",
        "    assert WRITER is not None, \"call configure to initialize SummaryWriter\"\n",
        "    WRITER.add_graph(model, input_to_model, verbose, **kwargs)\n",
        "\n",
        "\n",
        "def export_scalars(fname, overwrite=False):\n",
        "    assert WRITER is not None, \"call configure to initialize SummaryWriter\"\n",
        "    os.makedirs(os.path.join(WRITER.log_dir, 'scalar_data'), exist_ok=True)\n",
        "    if fname[-4:] != 'json':\n",
        "        fname += '.json'\n",
        "    fname = os.path.join(WRITER.log_dir, 'scalar_data', fname)\n",
        "    if not os.path.exists(fname) or overwrite:\n",
        "        WRITER.export_scalars_to_json(fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNBHXRvumGyY"
      },
      "outputs": [],
      "source": [
        "class Normal(D.Normal):\n",
        "    def mode(self):\n",
        "        return self.mean\n",
        "\n",
        "    def log_probs(self, action):\n",
        "        return super().log_prob(action).sum(-1, keepdim=True)\n",
        "\n",
        "    def entropy(self):\n",
        "        return super().entropy().sum(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPKIdx0dd6xj"
      },
      "outputs": [],
      "source": [
        "class AddBias(nn.Module):\n",
        "    def __init__(self, bias):\n",
        "        super(AddBias, self).__init__()\n",
        "        self._bias = nn.Parameter(bias.unsqueeze(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 2:\n",
        "            bias = self._bias.t().view(1, -1)\n",
        "        else:\n",
        "            bias = self._bias.t().view(1, -1, 1, 1)\n",
        "\n",
        "        return x + bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBr76nNedwJM"
      },
      "outputs": [],
      "source": [
        "def init(module, weight_init, bias_init, gain=1):\n",
        "    weight_init(module.weight.data, gain=gain)\n",
        "    bias_init(module.bias.data)\n",
        "    return module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AtgL_OSYVsE"
      },
      "outputs": [],
      "source": [
        "def update_linear_schedule(optimizer, update, total_num_updates, initial_lr):\n",
        "    \"\"\"Decreases the learning rate linearly\"\"\"\n",
        "    lr = initial_lr - (initial_lr * (update / float(total_num_updates)))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf_G9y-KUNse"
      },
      "outputs": [],
      "source": [
        "class DiagGaussian(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs):\n",
        "        super(DiagGaussian, self).__init__()\n",
        "\n",
        "        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n",
        "                               constant_(x, 0))\n",
        "\n",
        "        self.fc_mean = init_(nn.Linear(num_inputs, num_outputs))\n",
        "        self.logstd = AddBias(torch.zeros(num_outputs))\n",
        "\n",
        "    def forward(self, x):\n",
        "        action_mean = self.fc_mean(x)\n",
        "\n",
        "        #  An ugly hack for my KFAC implementation.\n",
        "        zeros = torch.zeros(action_mean.size())\n",
        "        if x.is_cuda:\n",
        "            zeros = zeros.cuda()\n",
        "\n",
        "        action_logstd = self.logstd(zeros)\n",
        "        return Normal(action_mean, action_logstd.exp())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyqQw3GGVyau"
      },
      "outputs": [],
      "source": [
        "class Rollouts(object):\n",
        "    def __init__(self,\n",
        "                 num_steps,\n",
        "                 num_processes,\n",
        "                 obs_shape,\n",
        "                 action_space,\n",
        "                 device=None,\n",
        "                 use_gae=False):\n",
        "\n",
        "        self.obs = torch.zeros(num_steps + 1, num_processes, *obs_shape)\n",
        "        self.masks = torch.ones(num_steps + 1, num_processes, 1)\n",
        "        self.value_preds = torch.zeros(num_steps + 1, num_processes, 1)\n",
        "        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n",
        "\n",
        "        self.rewards = torch.zeros(num_steps, num_processes, 1)\n",
        "        self.intrinsic_rewards = torch.zeros(num_steps, num_processes, 1)\n",
        "        self.selfish_extrinsic_rewards = torch.zeros(num_steps, num_processes, 1)\n",
        "        self.agentHolding = torch.zeros(num_steps, num_processes, 1)\n",
        "        self.action_log_probs = torch.zeros(num_steps, num_processes, 1)\n",
        "\n",
        "        if action_space.__class__.__name__ == 'Discrete':\n",
        "            action_shape = 1\n",
        "        else:\n",
        "            action_shape = action_space.shape[0]\n",
        "        self.actions = torch.zeros(num_steps, num_processes, action_shape)\n",
        "        if action_space.__class__.__name__ == 'Discrete':\n",
        "            self.actions = self.actions.long()\n",
        "\n",
        "        if device is not None:\n",
        "            self.device = device\n",
        "        else:\n",
        "            self.device = 'cpu'\n",
        "        self.to(self.device)\n",
        "\n",
        "        self.num_steps = num_steps\n",
        "        self.step = 0\n",
        "\n",
        "    def to(self, device=None):\n",
        "        if device is None:\n",
        "            device = self.device\n",
        "        self.obs = self.obs.to(device)\n",
        "        self.rewards = self.rewards.to(device)\n",
        "        self.intrinsic_rewards = self.intrinsic_rewards.to(device)\n",
        "        self.selfish_extrinsic_rewards = self.selfish_extrinsic_rewards.to(device)\n",
        "        self.value_preds = self.value_preds.to(device)\n",
        "        self.returns = self.returns.to(device)\n",
        "        self.action_log_probs = self.action_log_probs.to(device)\n",
        "        self.actions = self.actions.to(device)\n",
        "        self.agentHlding = self.agentHolding.to(device)\n",
        "        self.masks = self.masks.to(device)\n",
        "\n",
        "\n",
        "    def insert(self, obs, actions, action_log_probs, value_preds, rewards, intrinsic_rewards, selfish_extrinsic_rewards, masks, agentHolding):\n",
        "        self.obs[self.step + 1].copy_(obs)\n",
        "        self.actions[self.step].copy_(actions)\n",
        "        self.action_log_probs[self.step].copy_(action_log_probs)\n",
        "        self.value_preds[self.step].copy_(value_preds)\n",
        "        self.rewards[self.step].copy_(rewards)\n",
        "        self.intrinsic_rewards[self.step].copy_(intrinsic_rewards)\n",
        "        self.selfish_extrinsic_rewards[self.step].copy_(selfish_extrinsic_rewards)\n",
        "        self.agentHolding[self.step].copy_(agentHolding)\n",
        "        self.masks[self.step + 1].copy_(masks)\n",
        "\n",
        "        self.step = (self.step + 1) % self.num_steps\n",
        "\n",
        "    def after_update(self):\n",
        "        \"\"\"\n",
        "        After updating move the last observation and mask\n",
        "        to the begining of the rollout storage\n",
        "        \"\"\"\n",
        "        self.obs[0].copy_(self.obs[-1])\n",
        "        self.masks[0].copy_(self.masks[-1])\n",
        "\n",
        "    def compute_returns(self, next_value, gamma=0.99, use_gae=True, gae_lambda=0.95):\n",
        "\n",
        "        if use_gae:\n",
        "            self.value_preds[-1] = next_value\n",
        "            gae = 0\n",
        "            for step in reversed(range(self.rewards.size(0))):\n",
        "                delta = self.rewards[step] + gamma * self.value_preds[\n",
        "                    step + 1] * self.masks[step +\n",
        "                                           1] - self.value_preds[step]\n",
        "                gae = delta + gamma * gae_lambda * self.masks[step +\n",
        "                                                              1] * gae\n",
        "                self.returns[step] = gae + self.value_preds[step]\n",
        "        else:\n",
        "            self.returns[-1] = next_value\n",
        "            for step in reversed(range(self.rewards.size(0))):\n",
        "                self.returns[step] = self.returns[step + 1] * \\\n",
        "                    gamma * self.masks[step + 1] + self.rewards[step]\n",
        "\n",
        "\n",
        "    def feed_forward_generator(self, advantages, num_mini_batch):\n",
        "        # get number of steps and number of processes\n",
        "        num_steps, num_processes = self.rewards.size()[0:2]\n",
        "        batch_size = num_steps * num_processes\n",
        "        # make sure the size of the batch is greater than the number of mini batches\n",
        "        assert batch_size >= num_mini_batch\n",
        "        # size of minibatch is size of big batch / number of minibatches\n",
        "        mini_batch_size = batch_size // num_mini_batch\n",
        "        # This will randomly partition indices will keep the last partition even\n",
        "        # if it isn't the same size as mini_batch_size\n",
        "        sampler = BatchSampler(SubsetRandomSampler(range(batch_size)), mini_batch_size, drop_last=False)\n",
        "\n",
        "        for indices in sampler:\n",
        "            obs_batch = self.obs[:-1].view(-1, *self.obs.size()[2:])[indices]\n",
        "            actions_batch = self.actions.view(-1, *self.actions.size()[2:])[indices]\n",
        "            next_obs_batch = self.obs[1:].view(-1, *self.obs.size()[2:])[indices]\n",
        "            value_preds_batch = self.value_preds[:-1].view(-1, 1)[indices]\n",
        "            return_batch = self.returns[:-1].view(-1, 1)[indices]\n",
        "            masks_batch = self.masks[:-1].view(-1, 1)[indices]\n",
        "            old_action_log_probs_batch = self.action_log_probs.view(-1, 1)[indices]\n",
        "            adv_target = advantages.view(-1, 1)[indices]\n",
        "\n",
        "            yield obs_batch, actions_batch, next_obs_batch, value_preds_batch, return_batch, masks_batch, old_action_log_probs_batch, adv_target\n",
        "\n",
        "    def curiosity_generator(self, num_mini_batch):\n",
        "        # get number of steps and number of processes\n",
        "        num_steps, num_processes = self.rewards.size()[0:2]\n",
        "        batch_size = num_steps * num_processes\n",
        "        # make sure the size of the batch is greater than the number of mini batches\n",
        "        assert batch_size >= num_mini_batch\n",
        "        # size of minibatch is size of big batch / number of minibatches\n",
        "        mini_batch_size = batch_size // num_mini_batch\n",
        "        # This will randomly partition indices will keep the last partition even\n",
        "        # if it isn't the same size as mini_batch_size\n",
        "        sampler = BatchSampler(SubsetRandomSampler(range(batch_size)), mini_batch_size, drop_last=False)\n",
        "\n",
        "        for indices in sampler:\n",
        "            obs_batch = self.obs[:-1].view(-1, *self.obs.size()[2:])[indices]\n",
        "            next_obs_batch = self.obs[1:].view(-1, *self.obs.size()[2:])[indices]\n",
        "            actions_batch = self.actions.view(-1, *self.actions.size()[2:])[indices]\n",
        "\n",
        "            yield obs_batch, actions_batch, next_obs_batch\n",
        "\n",
        "class MultimodalRollouts(Rollouts):\n",
        "    def __init__(self, num_steps, num_processes,\n",
        "                 obs_shape, action_space, im_shape, depth_shape, contact_shape,\n",
        "                 device=None, use_gae=False):\n",
        "\n",
        "        self.image1 = torch.zeros(num_steps + 1, num_processes, *im_shape)\n",
        "        self.image2 = torch.zeros(num_steps + 1, num_processes, *im_shape)\n",
        "        self.depth1 = torch.zeros(num_steps + 1, num_processes, *depth_shape)\n",
        "        self.depth2 = torch.zeros(num_steps + 1, num_processes, *depth_shape)\n",
        "        self.contact = torch.zeros(num_steps + 1, num_processes, *contact_shape)\n",
        "        super(MultimodalRollouts, self).__init__(num_steps, num_processes, obs_shape, action_space, device, use_gae)\n",
        "\n",
        "        self.to()\n",
        "\n",
        "    def insert(self, obs, actions, action_log_probs,\n",
        "               value_preds, rewards, masks,\n",
        "               image1, image2, depth1, depth2, contact):\n",
        "\n",
        "        self.image1[self.step + 1].copy_(image1)\n",
        "        self.image2[self.step + 1].copy_(image2)\n",
        "        self.depth1[self.step + 1].copy_(depth1)\n",
        "        self.depth2[self.step + 1].copy_(depth2)\n",
        "        self.contact[self.step + 1].copy_(contact)\n",
        "        super(MultimodalRollouts, self).insert(obs, actions, action_log_probs,\n",
        "                                               alue_preds, rewards, masks)\n",
        "\n",
        "    def to(self):\n",
        "        self.image1 = self.image1.to(self.device)\n",
        "        self.image2 = self.image2.to(self.device)\n",
        "        self.depth1 = self.depth1.to(self.device)\n",
        "        self.depth2 = self.depth2.to(self.device)\n",
        "        self.contact = self.contact.to(self.device)\n",
        "        super(MultimodalRollouts, self).to(self.device)\n",
        "\n",
        "    def after_update(self):\n",
        "        \"\"\"\n",
        "        After updating move the last observation and mask\n",
        "        to the begining of the rollout storage\n",
        "        \"\"\"\n",
        "        self.obs[0].copy_(self.obs[-1])\n",
        "        self.masks[0].copy_(self.masks[-1])\n",
        "        self.image1[0].copy_(self.image1[-1])\n",
        "        self.image2[0].copy_(self.image2[-1])\n",
        "        self.depth1[0].copy_(self.depth1[-1])\n",
        "        self.depth2[0].copy_(self.depth2[-1])\n",
        "        self.contact[0].copy_(self.contact[-1])\n",
        "        super(MultimodalRollouts, self).after_update()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWKxEAmiYc8d"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, num_inputs, hidden_size, num_outputs):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.policy = Policy(num_inputs, hidden_size, num_outputs)\n",
        "        self.value_fn = ValueFn(num_inputs, hidden_size)\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def select_action(self, obs):\n",
        "        value, actor_features = self.value_fn(obs), self.policy(obs)\n",
        "        dist = self.policy.dist(actor_features)\n",
        "        action = dist.sample()\n",
        "        action_log_probs = dist.log_probs(action)\n",
        "        dist_entropy = dist.entropy().mean()\n",
        "        return value, action, action_log_probs\n",
        "\n",
        "    def evaluate_action(self, obs, action):\n",
        "        #print(\"Obs: \")\n",
        "        #print(obs)\n",
        "        value, actor_features = self.value_fn(obs), self.policy(obs)\n",
        "        #print(\"Value\")\n",
        "        #print(value)\n",
        "        #print(\"Actor features\")\n",
        "        #print(actor_features)\n",
        "        dist = self.policy.dist(actor_features)\n",
        "        action_log_probs = dist.log_probs(action)\n",
        "        entropy = dist.entropy().mean()\n",
        "\n",
        "        return value, action_log_probs, entropy\n",
        "\n",
        "    def get_value(self, obs):\n",
        "        return self.value_fn(obs)\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, num_inputs, hidden_size, num_outputs):\n",
        "        super(Policy, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        self.base = nn.Sequential(\n",
        "            nn.Linear(num_inputs, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh())\n",
        "        \"\"\"\n",
        "\n",
        "        self.base1 = nn.Linear(num_inputs, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.base2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.dist = DiagGaussian(hidden_size, num_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.tanh(self.base2(self.relu(self.base1(x))))\n",
        "        #return self.base(x)\n",
        "\n",
        "class ValueFn(nn.Module):\n",
        "    def __init__(self, num_inputs, hidden_size):\n",
        "        super(ValueFn, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        self.base = nn.Sequential(\n",
        "            nn.Linear(num_inputs, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU())\n",
        "        \"\"\"\n",
        "        self.base1 = nn.Linear(num_inputs, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.base2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.head = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.head(self.tanh(self.base2(self.relu(self.base1(x)))))\n",
        "        #return self.head(self.base(x))\n",
        "\n",
        "class FwdDyn(nn.Module):\n",
        "    def __init__(self, num_inputs, hidden_size, num_outputs):\n",
        "        super(FwdDyn, self).__init__()\n",
        "        self.base = nn.Sequential(\n",
        "            nn.Linear(num_inputs, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, num_outputs),\n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        feature = torch.cat((state, action), -1)\n",
        "        return self.base(feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pr73BGyR2ww"
      },
      "outputs": [],
      "source": [
        "class PPO():\n",
        "    def __init__(self,\n",
        "                 log_dir,\n",
        "                 observation_space,\n",
        "                 action_space,\n",
        "                 actor_critic=ActorCritic,\n",
        "                 dynamics_model=FwdDyn,\n",
        "                 optimizer=optim.Adam,\n",
        "                 hidden_size=64,\n",
        "                 num_steps=2048,\n",
        "                 num_processes=1,\n",
        "                 ppo_epochs=10,\n",
        "                 num_mini_batch=32,\n",
        "                 pi_lr=1e-4,\n",
        "                 v_lr=3e-4,\n",
        "                 dyn_lr=3e-4,\n",
        "                 clip_param=0.2,\n",
        "                 value_coef=0.5,\n",
        "                 entropy_coef=0.01,\n",
        "                 dyn_coef=0.5,\n",
        "                 grad_norm_max=0.5,\n",
        "                 use_clipped_value_loss=True,\n",
        "                 use_tensorboard=True,\n",
        "                 add_intrinsic_reward=False,\n",
        "                 add_selfish_extrinsic_reward=False,\n",
        "                 predict_delta_obs=False,\n",
        "                 device='cpu',\n",
        "                 share_optim=False,\n",
        "                 debug=False):\n",
        "\n",
        "        # setup logging\n",
        "        self.checkpoint_path = os.path.join(log_dir, 'checkpoint.pth')\n",
        "        self.checkpoint_path2 = os.path.join(log_dir, 'checkpoint2.pth')\n",
        "\n",
        "        # ppo hyperparameters\n",
        "        self.clip_param = clip_param\n",
        "        self.ppo_epochs = ppo_epochs\n",
        "        self.num_mini_batch = num_mini_batch\n",
        "\n",
        "        # loss hyperparameters\n",
        "        self.pi_lr = pi_lr\n",
        "        self.v_lr = v_lr\n",
        "        self.value_coef = value_coef\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.dyn_coef = dyn_coef\n",
        "\n",
        "        # clip values\n",
        "        self.grad_norm_max = grad_norm_max\n",
        "        self.use_clipped_value_loss = use_clipped_value_loss\n",
        "        self.add_intrinsic_reward = add_intrinsic_reward\n",
        "        self.add_selfish_extrinsic_reward = add_selfish_extrinsic_reward\n",
        "        self.predict_delta_obs = predict_delta_obs\n",
        "\n",
        "        # data normalization\n",
        "        self.obs_mean = None\n",
        "        self.obs_var = None\n",
        "\n",
        "        # setup actor critic\n",
        "        self.actor_critic = actor_critic(\n",
        "            num_inputs=observation_space.shape[0],\n",
        "            hidden_size=hidden_size,\n",
        "            num_outputs=action_space.shape[0])\n",
        "\n",
        "        # setup dynamics model\n",
        "        if self.add_intrinsic_reward:\n",
        "            dynamics_dim = observation_space.shape[0] + action_space.shape[0]\n",
        "            self.dynamics_model = dynamics_model(num_inputs=dynamics_dim,\n",
        "                                                 hidden_size=hidden_size,\n",
        "                                                 num_outputs=observation_space.shape[0])\n",
        "\n",
        "        # setup optimizers\n",
        "        self.share_optim = share_optim\n",
        "        if self.share_optim:\n",
        "            if self.add_intrinsic_reward:\n",
        "                self.optimizer = optimizer(list(self.actor_critic.parameters()) + list(self.dynamics_model.parameters()), lr=pi_lr)\n",
        "            else:\n",
        "                self.optimizer = optimizer(self.actor_critic.parameters(), lr=pi_lr)\n",
        "        else:\n",
        "            self.policy_optimizer = optimizer(self.actor_critic.policy.parameters(), lr=pi_lr)\n",
        "            self.value_fn_optimizer = optimizer(self.actor_critic.value_fn.parameters(), lr=v_lr)\n",
        "            if self.add_intrinsic_reward:\n",
        "                self.dynamics_optimizer = optimizer(self.dynamics_model.parameters(), lr=dyn_lr)\n",
        "\n",
        "        # create rollout storage\n",
        "        self.num_processes = num_processes\n",
        "        self.rollouts = Rollouts(num_steps, num_processes,\n",
        "                                 observation_space.shape,\n",
        "                                 action_space,\n",
        "                                 device)\n",
        "        \n",
        "        # teammate is set to None by default\n",
        "        self.teammate = None\n",
        "    \n",
        "    def setTeammate(self, tm):\n",
        "      self.teammate = tm\n",
        "\n",
        "    \n",
        "    def train(self):\n",
        "        self.actor_critic.train()\n",
        "        if self.add_intrinsic_reward:\n",
        "            self.dynamics_model.train()\n",
        "\n",
        "    def eval(self):\n",
        "        self.actor_critic.eval()\n",
        "        if self.add_intrinsic_reward:\n",
        "            self.dynamics_model.eval()\n",
        "\n",
        "    def select_action(self, step):\n",
        "        with torch.no_grad():\n",
        "            return self.actor_critic.select_action(self.rollouts.obs[step])\n",
        "\n",
        "    def evaluate_action(self, obs, action):\n",
        "        return self.actor_critic.evaluate_action(obs, action)\n",
        "\n",
        "    def get_value(self, obs):\n",
        "        with torch.no_grad():\n",
        "            return self.actor_critic.get_value(obs)\n",
        "\n",
        "    def store_rollout(self, obs, action, action_log_probs, value, reward, intrinsic_reward, selfish_extrinsic_reward, done, agentHolding):\n",
        "        masks = torch.tensor(1.0 - done.astype(np.float32)).view(-1, 1)\n",
        "        self.rollouts.insert(obs, action, action_log_probs, value, reward, intrinsic_reward, selfish_extrinsic_reward, masks, agentHolding)\n",
        "\n",
        "    def compute_returns(self, gamma, use_gae=True, gae_lambda=0.95):\n",
        "        with torch.no_grad():\n",
        "            next_value = self.actor_critic.get_value(self.rollouts.obs[-1]).detach()\n",
        "        if self.add_intrinsic_reward:\n",
        "            self.rollouts.rewards += self.rollouts.intrinsic_rewards\n",
        "        \n",
        "        # adding selfish extrinsic reward here\n",
        "        if self.add_selfish_extrinsic_reward:\n",
        "            self.rollouts.rewards += self.rollouts.selfish_extrinsic_rewards\n",
        "\n",
        "        self.rollouts.compute_returns(next_value, gamma, use_gae, gae_lambda)\n",
        "\n",
        "    def compute_intrinsic_reward(self, step):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            if self.teammate is None:\n",
        "              return 0\n",
        "            else:\n",
        "              obs = self.rollouts.obs[step]\n",
        "              action = self.rollouts.actions[step]\n",
        "              teammateAction = self.teammate.rollouts.actions[step]\n",
        "              next_obs = self.rollouts.obs[step + 1]\n",
        "              if self.predict_delta_obs:\n",
        "                  next_obs = (next_obs - obs)\n",
        "              next_obs_preds = self.teammate.dynamics_model(self.dynamics_model(obs, action), teammateAction)\n",
        "\n",
        "              # change to be l2 norm\n",
        "              return 0.5 * (next_obs_preds - next_obs).pow(2).sum(-1).unsqueeze(-1)\n",
        "        \n",
        "    def compute_selfish_extrinsic_reward(self, step):\n",
        "        with torch.no_grad():\n",
        "            if self.teammate is None:\n",
        "              return 0\n",
        "            else:\n",
        "              lastTouched = self.rollouts.agentHolding[step] # boolean if this agent was the last one to touch the hammer\n",
        "              beforeHeld = self.rollouts.agentHolding[step] or self.teammate.rollouts.agentHolding[step] # boolean if there is any arm holding the hammer\n",
        "              afterHeld = self.rollouts.agentHolding[step + 1] or self.teammate.rollouts.agentHolding[step + 1] # boolean if there is any arm holding the hammer\n",
        "\n",
        "              # reward if suddenly there is no arm holding hammer and this agent as the one that last touched\n",
        "              if lastTouched and (not beforeHeld) and afterHeld:\n",
        "                return 1\n",
        "              else:\n",
        "                return 0\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        "\n",
        "    def update(self, obs_mean, obs_var):\n",
        "        self.obs_mean = obs_mean\n",
        "        self.obs_var = obs_var\n",
        "        tot_loss, pi_loss, v_loss, dyn_loss, ent, kl, delta_p, delta_v = self._update()\n",
        "\n",
        "        self.rollouts.after_update()\n",
        "        return tot_loss, pi_loss, v_loss, dyn_loss, ent, kl, delta_p, delta_v\n",
        "\n",
        "    def compute_loss(self, sample):\n",
        "        # get sample batch\n",
        "        obs_batch, actions_batch, next_obs_batch, value_preds_batch, return_batch, masks_batch, old_action_log_probs_batch, adv_target = sample\n",
        "\n",
        "        '''print(\"Obs batch:\")\n",
        "        print(obs_batch)\n",
        "        print(\"Actions batch:\")\n",
        "        print(actions_batch)'''\n",
        "        # evaluate actions\n",
        "        values, action_log_probs, entropy = self.actor_critic.evaluate_action(obs_batch, actions_batch)\n",
        "\n",
        "        # compute policy loss\n",
        "        ratio = torch.exp(action_log_probs - old_action_log_probs_batch)\n",
        "        sur1 = ratio * adv_target\n",
        "        sur2 = torch.clamp(ratio, 1 - self.clip_param, 1 + self.clip_param) * adv_target\n",
        "        policy_loss = -torch.min(sur1, sur2).mean()\n",
        "\n",
        "        # compute value loss\n",
        "        if self.use_clipped_value_loss:\n",
        "            value_pred_clipped = value_preds_batch + (values - value_preds_batch).clamp(-self.clip_param, self.clip_param)\n",
        "            value_losses = (return_batch - values).pow(2).mean()\n",
        "            value_losses_clipped = (return_batch - value_pred_clipped).pow(2).mean()\n",
        "            value_loss = 0.5 * torch.max(value_losses, value_losses_clipped).mean()\n",
        "        else:\n",
        "            value_loss = 0.5 * (return_batch - values).pow(2).mean()\n",
        "\n",
        "        # compute dynamics loss\n",
        "        if self.add_intrinsic_reward:\n",
        "            dynamics_loss = self.compute_dynamics_loss(obs_batch, actions_batch, next_obs_batch, masks_batch)\n",
        "        else:\n",
        "            dynamics_loss = 0\n",
        "\n",
        "        # compute total loss\n",
        "        total_loss =  self.value_coef * value_loss + self.dyn_coef * dynamics_loss \\\n",
        "                    + (policy_loss - self.entropy_coef * entropy)\n",
        "\n",
        "        # compute kl divergence\n",
        "        kl = (old_action_log_probs_batch - action_log_probs).mean().detach()\n",
        "\n",
        "        return total_loss, policy_loss, value_loss, dynamics_loss, entropy, kl\n",
        "\n",
        "    def compute_dynamics_loss(self, obs, action, next_obs, masks):\n",
        "        if self.predict_delta_obs:\n",
        "            next_obs = (next_obs - obs)\n",
        "        next_obs_preds = self.dynamics_model(obs, action)\n",
        "        return 0.5 * (next_obs_preds - next_obs).pow(2).sum(-1).unsqueeze(-1).mean()\n",
        "\n",
        "    def _update(self):\n",
        "        # compute and normalize advantages\n",
        "        advantages = self.rollouts.returns[:-1] - self.rollouts.value_preds[:-1]\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-5)\n",
        "\n",
        "        # policy and value losses before gradient update\n",
        "        with torch.no_grad():\n",
        "            # Get whole batch of data\n",
        "            update_generator = self.rollouts.feed_forward_generator(advantages, num_mini_batch=1)\n",
        "            for update_sample in update_generator:\n",
        "                _, policy_loss_old, value_loss_old, _, _, _ = self.compute_loss(update_sample)\n",
        "\n",
        "        total_loss_epoch = 0\n",
        "        policy_loss_epoch = 0\n",
        "        value_loss_epoch = 0\n",
        "        dynamics_loss_epoch = 0\n",
        "        entropy_epoch = 0\n",
        "        kl_epoch = 0\n",
        "\n",
        "        for epoch in range(self.ppo_epochs):\n",
        "            data_generator = self.rollouts.feed_forward_generator(advantages, self.num_mini_batch)\n",
        "\n",
        "            for sample in data_generator:\n",
        "                total_loss, policy_loss, value_loss, dynamics_loss, entropy, kl = self.compute_loss(sample)\n",
        "\n",
        "                if self.share_optim:\n",
        "                    self.optimizer.zero_grad()\n",
        "                    total_loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), self.grad_norm_max)\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                    if not self.add_intrinsic_reward:\n",
        "                        dynamics_loss = torch.tensor(0).view(1, 1)\n",
        "                else:\n",
        "                    self.policy_optimizer.zero_grad()\n",
        "                    (policy_loss - self.entropy_coef * entropy).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.actor_critic.policy.parameters(), self.grad_norm_max)\n",
        "                    self.policy_optimizer.step()\n",
        "\n",
        "                    self.value_fn_optimizer.zero_grad()\n",
        "                    value_loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.actor_critic.value_fn.parameters(), self.grad_norm_max)\n",
        "                    self.value_fn_optimizer.step()\n",
        "\n",
        "                    if self.add_intrinsic_reward:\n",
        "                        self.dynamics_optimizer.zero_grad()\n",
        "                        dynamics_loss.backward()\n",
        "                        torch.nn.utils.clip_grad_norm_(self.dynamics_model.parameters(), self.grad_norm_max)\n",
        "                        self.dynamics_optimizer.step()\n",
        "                    else:\n",
        "                        dynamics_loss = torch.tensor(0).view(1, 1)\n",
        "\n",
        "                total_loss_epoch += total_loss.item()\n",
        "                policy_loss_epoch += policy_loss.item()\n",
        "                value_loss_epoch += value_loss.item()\n",
        "                dynamics_loss_epoch += dynamics_loss.item()\n",
        "                entropy_epoch += entropy.item()\n",
        "                kl_epoch += kl.item()\n",
        "\n",
        "        num_updates = (self.ppo_epochs + 1) * self.num_mini_batch\n",
        "        total_loss_epoch /= num_updates\n",
        "        policy_loss_epoch /= num_updates\n",
        "        value_loss_epoch /= num_updates\n",
        "        dynamics_loss_epoch /= num_updates\n",
        "        entropy_epoch /= num_updates\n",
        "        kl_epoch /= num_updates\n",
        "\n",
        "\n",
        "        # policy and value losses after gradient update\n",
        "        with torch.no_grad():\n",
        "            _, policy_loss_new, value_loss_new, _, _, _ = self.compute_loss(update_sample)\n",
        "            delta_p = policy_loss_new - policy_loss_old\n",
        "            delta_v = value_loss_new - value_loss_old\n",
        "\n",
        "        return total_loss_epoch, policy_loss_epoch, value_loss_epoch, dynamics_loss_epoch, entropy_epoch, kl_epoch, delta_p.item(), delta_v.item()\n",
        "    \n",
        "    def save_checkpoint(self, path=None):\n",
        "        # create checkpoint dict\n",
        "        checkpoint = {\n",
        "            'share_optim': self.share_optim,\n",
        "            'add_intrinsic_reward': self.add_intrinsic_reward,\n",
        "            'obs_mean': self.obs_mean,\n",
        "            'obs_var': self.obs_var}\n",
        "\n",
        "        # save models\n",
        "        checkpoint['actor_critic'] = self.actor_critic.state_dict()\n",
        "        if self.add_intrinsic_reward:\n",
        "            checkpoint['dynamics_model'] = self.dynamics_model.state_dict()\n",
        "\n",
        "        # save optimizer(s)\n",
        "        if self.share_optim:\n",
        "            checkpoint['optimizer'] = self.optimizer.state_dict()\n",
        "        else:\n",
        "            checkpoint['policy_optimizer'] = self.policy_optimizer.state_dict()\n",
        "            checkpoint['value_fn_optimizer'] = self.value_fn_optimizer.state_dict()\n",
        "            if self.add_intrinsic_reward:\n",
        "                checkpoint['dynamics_model'] = self.dynamics_model.state_dict()\n",
        "                checkpoint['dynamics_optimizer'] = self.dynamics_optimizer.state_dict()\n",
        "\n",
        "        if path is None:\n",
        "            torch.save(checkpoint, self.checkpoint_path)\n",
        "            torch.save(self.actor_critic, self.checkpoint_path2)\n",
        "        else:\n",
        "            torch.save(checkpoint, path)\n",
        "    \n",
        "    def load_checkpoint(self, path):\n",
        "        # load checkpoint\n",
        "        checkpoint = torch.load(path)\n",
        "        self.share_optim = checkpoint['share_optim']\n",
        "        self.add_intrinsic_reward = checkpoint['add_intrinsic_reward']\n",
        "        self.obs_mean = checkpoint['obs_mean']\n",
        "        self.obs_var = checkpoint['obs_var']\n",
        "\n",
        "        # load models\n",
        "        self.actor_critic.load_state_dict(checkpoint['actor_critic'])\n",
        "        if self.add_intrinsic_reward:\n",
        "            self.dynamics_model.load_state_dict(checkpoint['dynamics_model'])\n",
        "\n",
        "        # load optimizer(s)\n",
        "        if self.share_optim:\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        else:\n",
        "            self.policy_optimizer.load_state_dict(checkpoint['policy_optimizer'])\n",
        "            self.value_fn_optimizer.load_state_dict(checkpoint['value_fn_optimizer'])\n",
        "            if self.add_intrinsic_reward:\n",
        "                self.dynamics_optimizer.load_state_dict(checkpoint['dynamics_optimizer'])\n",
        "\n",
        "    def load_models(self, path):\n",
        "        checkpoint = torch.load(path)\n",
        "        # load models\n",
        "        self.actor_critic.load_state_dict(checkpoint['actor_critic'])\n",
        "        if self.add_intrinsic_reward:\n",
        "            self.dynamics_model.load_state_dict(checkpoint['dynamics_model'])\n",
        "        del checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNmzmIH7LR8N"
      },
      "outputs": [],
      "source": [
        "def return_observation_values(base):\n",
        "    count = 1e-4\n",
        "    mean = np.zeros(base.shape, 'float64')\n",
        "    var = np.ones(base.shape, 'float64')\n",
        "    batch_mean = np.mean(base, axis=0)\n",
        "    batch_var = np.var(base, axis=0)\n",
        "    batch_count = base.shape[0]\n",
        "\n",
        "    delta = batch_mean - mean\n",
        "    tot_count = count + batch_count\n",
        "\n",
        "    new_mean = mean + delta * batch_count / tot_count\n",
        "    m_a = var * count\n",
        "    m_b = batch_var * batch_count\n",
        "    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
        "    new_var = M2 / tot_count\n",
        "    new_count = tot_count\n",
        "\n",
        "    base = torch.tensor(base.reshape((1, len(base))))\n",
        "    base_clip = torch.tensor(np.clip((base - new_mean) / np.sqrt(new_var + new_count), -10., 10.))\n",
        "    base_mean = torch.tensor(new_mean.reshape((1, len(new_mean))))\n",
        "    base_var = torch.tensor(new_var.reshape((1, len(new_var))))\n",
        "\n",
        "    return np.array([base, base_clip, base_mean, base_var])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svkHQoI0Oba4"
      },
      "outputs": [],
      "source": [
        "def form_observations(obs):\n",
        "    agent1_observation_space = np.empty(87)\n",
        "    agent2_observation_space = np.empty(87)\n",
        "\n",
        "    start1 = 0\n",
        "    start2 = 0\n",
        "    for key in obs:\n",
        "        if \"robot0\" in key:\n",
        "            agent1_observation_space[start1:start1 + len(obs[key])] = obs[key]\n",
        "            start1 += len(obs[key])\n",
        "        elif \"robot1\" in key:\n",
        "            agent2_observation_space[start2:start2 + len(obs[key])] = obs[key]\n",
        "            start2 += len(obs[key])\n",
        "        elif \"hammer\" in key:\n",
        "            agent1_observation_space[start1:start1 + len(obs[key])] = obs[key]\n",
        "            start1 += len(obs[key])\n",
        "            agent2_observation_space[start2:start2 + len(obs[key])] = obs[key]\n",
        "            start2 += len(obs[key])\n",
        "        elif \"object\" in key:\n",
        "            agent1_observation_space[start1:start1 + len(obs[key])] = obs[key]\n",
        "            start1 += len(obs[key])\n",
        "            agent2_observation_space[start2:start2 + len(obs[key])] = obs[key]\n",
        "            start2 += len(obs[key])\n",
        "\n",
        "    agent1_complete_space = return_observation_values(agent1_observation_space)\n",
        "    agent2_complete_space = return_observation_values(agent2_observation_space)\n",
        "\n",
        "    return agent1_observation_space, agent1_complete_space, agent2_observation_space, agent2_complete_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCwosjCYEOLm"
      },
      "outputs": [],
      "source": [
        "# create environment instance\n",
        "\n",
        "configure('/content', tbX = True)\n",
        "\n",
        "env = suite.make(\n",
        "    env_name=\"TwoArmHandover\", # try with other tasks like \"Stack\" and \"Door\"\n",
        "    robots=[\"Panda\", \"Panda\"],  # try with other robots like \"Sawyer\" and \"Jaco\"\n",
        "    use_object_obs=True,\n",
        "    ignore_done=True\n",
        ")\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "torch.set_num_threads(1)\n",
        "\n",
        "# reset the environment\n",
        "obs = env.reset()\n",
        "\n",
        "# prepare observation format\n",
        "agent1_observation_space, agent1_complete_space, agent2_observation_space, agent2_complete_space = form_observations(obs)\n",
        "\n",
        "# experiment switches\n",
        "use_intrinsic_reward = True\n",
        "max_intrinsic_reward = None\n",
        "intrinsic_coefficient = 1\n",
        "selfish_extrinsic_coefficient = -1\n",
        "use_selfish_extrinsic_reward = True\n",
        "\n",
        "agent1 = PPO('/content', agent1_observation_space, env.action_spec[0], add_intrinsic_reward=use_intrinsic_reward, add_selfish_extrinsic_reward=use_selfish_extrinsic_reward)\n",
        "agent2 = PPO('/content', agent2_observation_space, env.action_spec[1], add_intrinsic_reward=use_intrinsic_reward, add_selfish_extrinsic_reward=use_selfish_extrinsic_reward)\n",
        "\n",
        "agent1.setTeammate(agent2)\n",
        "agent2.setTeammate(agent1)\n",
        "\n",
        "agents = [agent1, agent2]\n",
        "\n",
        "num_env_steps = 1000 #1e07\n",
        "num_steps = 2 #2048\n",
        "num_updates = num_env_steps // agent1.num_processes // num_steps\n",
        "\n",
        "gamma = 0.9\n",
        "use_gae = True\n",
        "gae_lambda = 0.95\n",
        "\n",
        "# start training\n",
        "log_interval = 1\n",
        "checkpoint_interval = 20\n",
        "use_tensorboard = True\n",
        "start = time.time()\n",
        "\n",
        "agent_index = -1\n",
        "for agent in agents:\n",
        "    agent.actor_critic = agent.actor_critic.to(device)\n",
        "    if use_intrinsic_reward:\n",
        "        agent.dynamics_model = agent.dynamics_model.to(device)\n",
        "    agent_index += 1\n",
        "    if agent_index == 0:\n",
        "        obs = agent1_complete_space\n",
        "    else:\n",
        "        obs = agent2_complete_space\n",
        "    agent.rollouts.obs[0].copy_(obs[1])\n",
        "    agent.rollouts.to(device)\n",
        "    agent.train()\n",
        "\n",
        "for update in range(num_env_steps):\n",
        "    print(update)\n",
        "\n",
        "    for optimizer in [agent1.policy_optimizer, agent1.value_fn_optimizer]:\n",
        "        update_linear_schedule(optimizer=optimizer,\n",
        "                                  update=update,\n",
        "                                  total_num_updates=num_updates,\n",
        "                                  initial_lr=agent1.pi_lr)\n",
        "\n",
        "    agent1_extrinsic_rewards = []\n",
        "    agent2_extrinsic_rewards = []\n",
        "    episode_length = []\n",
        "    agent1_intrinsic_rewards = []\n",
        "    agent2_intrinsic_rewards = []\n",
        "    solved_episodes = []\n",
        "    agent1_selfish_extrinsic_rewards = []\n",
        "    agent2_selfish_extrinsic_rewards = []\n",
        "\n",
        "    for step in range(num_steps):\n",
        "\n",
        "        agent_index = -1\n",
        "        for agent in agents:\n",
        "            agent_index += 1\n",
        "            # select action\n",
        "            value, action, action_log_probs = agent.select_action(step)\n",
        "\n",
        "            # take a step in the environment\n",
        "            obs, reward, done, infos = env.step(action[0].cpu().detach().numpy())\n",
        "\n",
        "            # prepare observations\n",
        "            agent1_observation_space, agent1_complete_space, agent2_observation_space, agent2_complete_space = form_observations(obs)\n",
        "            if agent_index == 0:\n",
        "                obs = agent1_complete_space\n",
        "            else:\n",
        "                obs = agent2_complete_space\n",
        "\n",
        "            # calculate intrinsic reward\n",
        "            if use_intrinsic_reward:\n",
        "                intrinsic_reward = intrinsic_coefficient * agent.compute_intrinsic_reward(step)\n",
        "                if max_intrinsic_reward is not None:\n",
        "                    intrinsic_reward = torch.clamp(agent.compute_intrinsic_reward(step), 0.0, max_intrinsic_reward)\n",
        "            else:\n",
        "                intrinsic_reward = torch.tensor(0).view(1, 1)\n",
        "            \n",
        "            # handle extrinsic reward\n",
        "            reward = torch.tensor(reward)\n",
        "\n",
        "            # calculate selfish extrinsic reward\n",
        "            selfish_extrinsic_reward = 0 # simply declaring variable\n",
        "            if use_selfish_extrinsic_reward:\n",
        "                selfish_extrinsic_reward = selfish_extrinsic_coefficient * agent.compute_selfish_extrinsic_reward(step)\n",
        "            \n",
        "            # handle selfish extrinsic reward\n",
        "            selfish_extrinsic_reward = torch.tensor(selfish_extrinsic_reward)\n",
        "\n",
        "\n",
        "            # save both types of rewards\n",
        "            if agent_index == 0:\n",
        "                agent1_intrinsic_rewards.extend(list(intrinsic_reward.cpu().numpy().reshape(-1)))\n",
        "                agent1_extrinsic_rewards.extend(list(reward.cpu().numpy().reshape(-1)))\n",
        "                agent1_selfish_extrinsic_rewards.extend(list(selfish_extrinsic_reward.cpu().numpy().reshape(-1)))\n",
        "            else:\n",
        "                agent2_intrinsic_rewards.extend(list(intrinsic_reward.cpu().numpy().reshape(-1)))\n",
        "                agent2_extrinsic_rewards.extend(list(reward.cpu().numpy().reshape(-1)))\n",
        "                agent2_selfish_extrinsic_rewards.extend(list(selfish_extrinsic_reward.cpu().numpy().reshape(-1)))\n",
        "\n",
        "            # store experience\n",
        "            done_value = np.zeros(1)\n",
        "            if done:\n",
        "                done_value = np.ones(1)\n",
        "\n",
        "            # Check if any Arm's gripper is grasping the hammer handle\n",
        "            curGripper = env.robots[agent_index].gripper\n",
        "            agentHolding = env._check_grasp(gripper=curGripper, object_geoms=env.hammer)\n",
        "            agentHolding = torch.tensor(agentHolding)\n",
        "\n",
        "            agent.store_rollout(obs[1], action, action_log_probs, value, reward, intrinsic_reward, selfish_extrinsic_reward, done_value, agentHolding)\n",
        "    \n",
        "    agent_index = -1\n",
        "    for agent in agents:\n",
        "        agent_index += 1\n",
        "        # compute returns\n",
        "        agent.compute_returns(gamma, use_gae, gae_lambda)\n",
        "        # update policy and value_fn, reset rollout storage\n",
        "        tot_loss, pi_loss, v_loss, dyn_loss, entropy, kl, delta_p, delta_v =  agent.update(obs_mean=obs[2], obs_var=obs[3])\n",
        "\n",
        "    # new episode\n",
        "    if done:\n",
        "        env = suite.make(\n",
        "            env_name=\"TwoArmHandover\", # try with other tasks like \"Stack\" and \"Door\"\n",
        "            robots=[\"Panda\", \"Panda\"],  # try with other robots like \"Sawyer\" and \"Jaco\"\n",
        "            use_object_obs=True,\n",
        "            ignore_done=True\n",
        "        )\n",
        "        obs = env.reset()\n",
        "        \n",
        "    # checkpoint model\n",
        "    if (update + 1) % checkpoint_interval == 0:\n",
        "        agent.save_checkpoint()\n",
        "\n",
        "    # log data\n",
        "    if update % log_interval == 0:\n",
        "        current = time.time()\n",
        "        elapsed = current - start\n",
        "        total_steps = (update + 1) * agent.num_processes * num_steps\n",
        "        fps =int(total_steps / (current - start))\n",
        "\n",
        "        logkv('Time/Updates', update)\n",
        "        logkv('Time/Total Steps', total_steps)\n",
        "        logkv('Time/FPS', fps)\n",
        "        logkv('Time/Current', current)\n",
        "        logkv('Time/Elapsed', elapsed)\n",
        "        logkv('Time/Epoch', elapsed)\n",
        "        if agent_index == 0:\n",
        "            extrinsic_rewards = agent1_extrinsic_rewards\n",
        "            intrinsic_rewards = agent1_intrinsic_rewards\n",
        "            selfish_extrinsic_rewards = agent1_selfish_extrinsic_rewards\n",
        "        else:\n",
        "            extrinsic_rewards = agent2_extrinsic_rewards\n",
        "            intrinsic_rewards = agent2_intrinsic_rewards\n",
        "            selfish_extrinsic_rewards = agent2_selfish_extrinsic_rewards\n",
        "\n",
        "        logkv('Extrinsic/Mean', np.mean(extrinsic_rewards))\n",
        "        logkv('Extrinsic/Median', np.median(extrinsic_rewards))\n",
        "        logkv('Extrinsic/Min', np.min(extrinsic_rewards))\n",
        "        logkv('Extrinsic/Max', np.max(extrinsic_rewards))\n",
        "        logkv('SelfishExtrinsic/Mean', np.mean(selfish_extrinsic_rewards))\n",
        "        logkv('SelfishExtrinsic/Median', np.median(selfish_extrinsic_rewards))\n",
        "        logkv('SelfishExtrinsic/Min', np.min(selfish_extrinsic_rewards))\n",
        "        logkv('SelfishExtrinsic/Max', np.max(selfish_extrinsic_rewards))\n",
        "        logkv('Intrinsic/Mean', np.mean(intrinsic_rewards))\n",
        "        logkv('Intrinsic/Median', np.median(intrinsic_rewards))\n",
        "        logkv('Intrinsic/Min', np.min(intrinsic_rewards))\n",
        "        logkv('Intrinsic/Max', np.max(intrinsic_rewards))\n",
        "        logkv('Loss/Total', tot_loss)\n",
        "        logkv('Loss/Policy', pi_loss)\n",
        "        logkv('Loss/Value', v_loss)\n",
        "        logkv('Loss/Entropy', entropy)\n",
        "        logkv('Loss/KL', kl)\n",
        "        logkv('Loss/DeltaPi', delta_p)\n",
        "        logkv('Loss/DeltaV', delta_v)\n",
        "        logkv('Loss/Dynamics', dyn_loss)\n",
        "        logkv('Value/Mean', np.mean(agent.rollouts.value_preds.cpu().data.numpy()))\n",
        "        logkv('Value/Median', np.median(agent.rollouts.value_preds.cpu().data.numpy()))\n",
        "        logkv('Value/Min', np.min(agent.rollouts.value_preds.cpu().data.numpy()))\n",
        "        logkv('Value/Max', np.max(agent.rollouts.value_preds.cpu().data.numpy()))\n",
        "\n",
        "        if use_tensorboard:\n",
        "            add_scalar('reward/mean', np.mean(extrinsic_rewards), total_steps, elapsed)\n",
        "            add_scalar('reward/median', np.median(extrinsic_rewards), total_steps, elapsed)\n",
        "            add_scalar('reward/min', np.min(extrinsic_rewards), total_steps, elapsed)\n",
        "            add_scalar('reward/max', np.max(extrinsic_rewards), total_steps, elapsed)\n",
        "            add_scalar('reward/mean', np.mean(selfish_extrinsic_rewards), total_steps, elapsed)\n",
        "            add_scalar('reward/median', np.median(selfish_extrinsic_rewards), total_steps, elapsed)\n",
        "            add_scalar('reward/min', np.min(selfish_extrinsic_rewards), total_steps, elapsed)\n",
        "            add_scalar('reward/max', np.max(selfish_extrinsic_rewards), total_steps, elapsed)\n",
        "            add_scalar('intrinsic/mean', np.mean(intrinsic_rewards), total_steps, elapsed)\n",
        "            add_scalar('intrinsic/median', np.median(intrinsic_rewards), total_steps, elapsed)\n",
        "            add_scalar('intrinsic/min', np.min(intrinsic_rewards), total_steps, elapsed)\n",
        "            add_scalar('intrinsic/max', np.max(intrinsic_rewards), total_steps, elapsed)\n",
        "            add_scalar('loss/total', tot_loss, total_steps, elapsed)\n",
        "            add_scalar('loss/policy', pi_loss, total_steps, elapsed)\n",
        "            add_scalar('loss/value', v_loss, total_steps, elapsed)\n",
        "            add_scalar('loss/entropy', entropy, total_steps, elapsed)\n",
        "            add_scalar('loss/kl', kl, total_steps, elapsed)\n",
        "            add_scalar('loss/delta_p', delta_p, total_steps, elapsed)\n",
        "            add_scalar('loss/delta_v', delta_v, total_steps, elapsed)\n",
        "            add_scalar('loss/dynamics', dyn_loss, total_steps, elapsed)\n",
        "            add_scalar('value/mean', np.mean(agent.rollouts.value_preds.cpu().data.numpy()), total_steps, elapsed)\n",
        "            add_scalar('value/median', np.median(agent.rollouts.value_preds.cpu().data.numpy()), total_steps, elapsed)\n",
        "            add_scalar('value/min', np.min(agent.rollouts.value_preds.cpu().data.numpy()), total_steps, elapsed)\n",
        "            add_scalar('value/max', np.max(agent.rollouts.value_preds.cpu().data.numpy()), total_steps, elapsed)\n",
        "\n",
        "            dumpkvs()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GyCxqCDX1o7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "JDZe92xI1w06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPjz_q6cxj59"
      },
      "outputs": [],
      "source": [
        "!tensorboard --logdir=content"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Combining Selfish Individual Rewards With Intrinsic Motivation For Synergistic Tasks.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}